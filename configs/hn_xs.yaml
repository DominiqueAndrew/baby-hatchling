seed: 17
threads: 16
model:
  name: hn_xs
  d_model: 512
  n_layers: 16                    # 12 Spike-KDA + 4 NoPE
  n_heads: 8
  dk: 64
  dv: 64
  d_ff: 2048
  rank_gate: 256
  group_kv: 1
  gw_tokens: 0
  max_seq: 2048
  spike_decay: 0.92
  spike_threshold: 0.35
  spike_surrogate_beta: 9.0
  episodic_bytes: 16777216
  use_predictive_head: true
  use_episodic_memory: false        # Disabled - causes 10-20x slowdown due to GPUâ†’CPU transfers
  use_curiosity_bonus: true
  use_gradient_checkpointing: true  # Re-enabled with optimizations
  kda_chunk_size: 16                # Chunk size for KDA parallel processing (8-32 recommended)
  kda_mode: scan                    # use Blelloch parallel scan on GPU
  kda_scan_min_len: 64
  token_drop:
    enabled: true                    # Re-enabled with optimizations
    prob: 0.25
    min_layer: 4
    max_layer: 14
train:
  pretrain:
    seq_len: 1024                  # fallback when curriculum completes
    batch_tokens: 6144             # Reduced for safety
    grad_accum: 16
    grad_clip: 1.0
    max_steps: 60000               # Reduced from 120000 (50% reduction with optimizations)
    sparsity:
      enabled: true                 # Re-enabled with optimizations
      target: 0.7
      warmup: 5000
      prune: 20000
      restore: 10000
      update_every: 1000
    curriculum:
      - steps: 10000
        seq_len: 256
        batch_tokens: 4096          # batch_size=16 (increased for better GPU utilization)
        grad_accum: 12              # Reduced to maintain effective batch
      - steps: 10000
        seq_len: 512
        batch_tokens: 5120          # batch_size=10 (increased)
        grad_accum: 12
      - steps: 15000
        seq_len: 768
        batch_tokens: 4608          # batch_size=6 (increased)
        grad_accum: 16
      - steps: 25000
        seq_len: 1024
        batch_tokens: 5120          # batch_size=5 (increased)
        grad_accum: 20
    lr_schedule:
      type: cosine
      warmup_steps: 2000           # Reduced from 3000
      min_factor: 0.1
    early_stopping:
      enabled: true
      patience: 5000               # Reduced from 8000 (stops sooner if no improvement)
      min_delta: 0.02
  sft:
    seq_len: 2048
    batch_tokens: 16384
    grad_accum: 16
    grad_clip: 1.0
    max_steps: 8000
optim:
  type: adapm
  lr: 0.0002
  beta: 0.9
  gamma: 0.3
  eps: 1.0e-08
  weight_decay: 0.02
loss:
  lambda_pc: 0.2
  lambda_epi: 0.02
  lambda_kl: 0.02
rlvr:
  steps: 600
  epsilon_clip: 0.2
  rho_max: 4.0
  samples_per_prompt: 2
  temperature: 0.8
  kl_target: 0.02
datasets:
  pretrain:
    - name: wikipedia_en
      hf_id: Salesforce/wikitext
      config: wikitext-103-v1
      split: train
    - name: wikitext2
      hf_id: Salesforce/wikitext
      config: wikitext-2-v1
      split: train
    - name: python_code_humaneval
      hf_id: openai/openai_humaneval
      split: test
      limit: 164
      field: prompt
    - name: python_code_solutions
      hf_id: openai/openai_humaneval
      split: test
      limit: 164
      field: canonical_solution
    - name: gsm8k_questions
      hf_id: openai/gsm8k
      config: main
      split: train
      limit: 4000
      field: question
    - name: gsm8k_answers
      hf_id: openai/gsm8k
      config: main
      split: train
      limit: 4000
      field: answer
    - name: alpaca_combined
      hf_id: tatsu-lab/alpaca
      split: train
      limit: 5000
      template: |
        Instruction: {instruction}
        Input: {input}
        Response: {output}
    - name: meta_math_combined
      hf_id: meta-math/MetaMathQA
      split: train
      limit: 5000
      template: |
        Question: {query}
        Answer: {response}
    - name: hellaswag_combined
      hf_id: Rowan/hellaswag
      split: train
      limit: 3000
      template: |
        Context: {ctx}
        Endings: {endings}
    - name: winogrande_combined
      hf_id: winogrande
      config: winogrande_xl
      split: train
      limit: 2000
      template: |
        Sentence: {sentence}
        Option 1: {option1}
        Option 2: {option2}
    - name: crawler_shards
      path: data/crawler/shards/*.jsonl
  sft:
    - name: alpaca_clean
      hf_id: tatsu-lab/alpaca
      split: train
      template: |
        Instruction: {instruction}
        Input: {input}
        Response: {output}
    - name: openhermes_en
      hf_id: teknium/OpenHermes-2.5
      split: train
      limit: 5000
  math:
    - name: gsm8k_mini
      hf_id: openai/gsm8k
      split: train
      limit: 4000
  code:
    - name: humaneval_plus
      hf_id: evalplus/humanevalplus
      split: test
      limit: 164
probes:
  palindrome:
    lengths: [64, 256, 512]
  mqar:
    lengths: [128, 256, 512]
  stack:
    depths: [8, 16, 32]
gridworld:
  size: 5
  max_steps: 24
  episodes: 8
  policy: greedy
  log_path: logs/gridworld_hn_xs.csv
